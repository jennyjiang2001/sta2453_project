{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e21b2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af86e94b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calanoid_1:\n",
      "  HURON: 85519\n",
      "  SIMC: 215214\n",
      "\n",
      "Cyclopoid_1:\n",
      "  HURON: 7686\n",
      "  SIMC: 197982\n",
      "\n",
      "Bosmina_1:\n",
      "  HURON: 4855\n",
      "  SIMC: 2875\n",
      "\n",
      "Harpacticoida:\n",
      "  HURON: 0\n",
      "  SIMC: 0\n",
      "\n",
      "Chironomid:\n",
      "  HURON: 2072\n",
      "  SIMC: 38\n",
      "\n",
      "Chydoridae:\n",
      "  HURON: 157\n",
      "  SIMC: 43\n",
      "\n",
      "Daphnia:\n",
      "  HURON: 30\n",
      "  SIMC: 558\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_folders = {\n",
    "    \"HURON\": \"./HURONOvlerap_csv\",  \n",
    "    \"SIMC\": \"./SIMCOverlap_csv\"    \n",
    "}\n",
    "\n",
    "categories = [\"Calanoid_1\", \"Cyclopoid_1\", \"Bosmina_1\", \n",
    "              \"Harpacticoida\", \"Chironomid\", \"Chydoridae\", \"Daphnia\"]\n",
    "\n",
    "category_counts = {category: {\"HURON\": 0, \"SIMC\": 0} for category in categories}\n",
    "\n",
    "for folder_name, folder_path in data_folders.items():\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith(\".csv\"):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            if \"Class\" in df.columns:\n",
    "\n",
    "                for category in categories:\n",
    "                    count = df[df[\"Class\"] == category].shape[0]\n",
    "                    category_counts[category][folder_name] += count\n",
    "\n",
    "for category, counts in category_counts.items():\n",
    "    print(f\"{category}:\")\n",
    "    print(f\"  HURON: {counts['HURON']}\")\n",
    "    print(f\"  SIMC: {counts['SIMC']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e2fe43",
   "metadata": {},
   "source": [
    "## filter target classes entries & add file time in the first column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a2490c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_classes = [\"Calanoid_1\", \"Cyclopoid_1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7db0fd68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files have been processed and saved to ./combined_filtered_data_HURON.csv!\n",
      "All files have been processed and saved to ./combined_filtered_data_SIMC.csv!\n"
     ]
    }
   ],
   "source": [
    "def combine_filtered_data(input_folder, output_file, target_classes):\n",
    "    combined_data = pd.DataFrame()\n",
    "\n",
    "    for file_name in os.listdir(input_folder):\n",
    "        input_file_path = os.path.join(input_folder, file_name)\n",
    "\n",
    "        if not file_name.endswith(\".csv\"):\n",
    "            continue\n",
    "\n",
    "        data = pd.read_csv(input_file_path)\n",
    "\n",
    "        filtered_data = data[data[\"Class\"].isin(target_classes)]\n",
    "\n",
    "        filtered_data.insert(0, 'file_name', file_name)\n",
    "\n",
    "        combined_data = pd.concat([combined_data, filtered_data], ignore_index=True)\n",
    "\n",
    "    combined_data.to_csv(output_file, index=False)\n",
    "    print(f\"All files have been processed and saved to {output_file}!\")\n",
    "\n",
    "combine_filtered_data(\"./HURONOvlerap_csv\", \"./combined_filtered_data_HURON.csv\", target_classes)\n",
    "combine_filtered_data(\"./SIMCOverlap_csv\", \"./combined_filtered_data_SIMC.csv\", target_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a120aaa1",
   "metadata": {},
   "source": [
    "## keep distinct entries in MasterTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8bb62c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# rows in original MasterTable: 17807 \n",
      "# distinct rows in updated MasterTable: 17075 \n"
     ]
    }
   ],
   "source": [
    "file_path = \"MasterTable_AI_FlowCAM.xlsx\"\n",
    "master_table = pd.read_excel(file_path)\n",
    "\n",
    "master_table_distinct = master_table.drop_duplicates()\n",
    "\n",
    "output_file = \"MasterTable_Distinct.xlsx\"\n",
    "master_table_distinct.to_excel(output_file, index=False)\n",
    "\n",
    "print(f\"# rows in original MasterTable: {len(master_table)} \")\n",
    "print(f\"# distinct rows in updated MasterTable: {len(master_table_distinct)} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918b641d",
   "metadata": {},
   "source": [
    "## use \"tifffile & csvfile\" as key to add MasterTable variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "547f99a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "find 692 repeated 'tifffile & csvfile' combinations\n",
      "repeated 'tifffile & csvfile' combinations saved to 'Duplicate_Tifffile_Csvfile_Records.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = \"MasterTable_Distinct.xlsx\"\n",
    "master_table = pd.read_excel(file_path)\n",
    "\n",
    "duplicate_groups = master_table.groupby([\"tifffile\", \"csvfile\"]).size().reset_index(name=\"count\")\n",
    "\n",
    "duplicates = duplicate_groups[duplicate_groups[\"count\"] > 1]\n",
    "\n",
    "if duplicates.empty:\n",
    "    print(\"all 'tifffile & csvfile' combinations are unique\")\n",
    "else:\n",
    "    print(f\"find {len(duplicates)} repeated 'tifffile & csvfile' combinations\")\n",
    "\n",
    "    duplicate_rows = master_table[master_table.duplicated(subset=[\"tifffile\", \"csvfile\"], keep=False)]\n",
    "    duplicate_rows_with_index = duplicate_rows.reset_index() \n",
    "\n",
    "    duplicate_rows_with_index.to_csv(\"Duplicate_Tifffile_Csvfile_Records.csv\", index=False)\n",
    "    print(\"repeated 'tifffile & csvfile' combinations saved to 'Duplicate_Tifffile_Csvfile_Records.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf62426",
   "metadata": {},
   "source": [
    "### find variables with different entries in unique combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d16ee3eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "find 692 repeated 'tifffile & csvfile' combinations\n",
      "varibales are \n",
      "['YPerchDen']\n"
     ]
    }
   ],
   "source": [
    "file_path = \"MasterTable_Distinct.xlsx\"\n",
    "master_table = pd.read_excel(file_path)\n",
    "\n",
    "duplicate_groups = master_table.groupby([\"tifffile\", \"csvfile\"]).size().reset_index(name=\"count\")\n",
    "duplicates = duplicate_groups[duplicate_groups[\"count\"] > 1]\n",
    "\n",
    "if duplicates.empty:\n",
    "    print(\"all 'tifffile & csvfile' combinations are unique\")\n",
    "else:\n",
    "    print(f\"find {len(duplicates)} repeated 'tifffile & csvfile' combinations\")\n",
    "\n",
    "    duplicate_rows = master_table[master_table.duplicated(subset=[\"tifffile\", \"csvfile\"], keep=False)]\n",
    "\n",
    "    grouped = duplicate_rows.groupby([\"tifffile\", \"csvfile\"])\n",
    "\n",
    "    different_columns = set()\n",
    "    \n",
    "    for (tifffile, csvfile), group in grouped:\n",
    "        group = group.drop(columns=[\"tifffile\", \"csvfile\"]).reset_index(drop=True)\n",
    "        \n",
    "        for col in group.columns:\n",
    "            if group[col].nunique() > 1:  \n",
    "                different_columns.add(col)\n",
    "\n",
    "    if different_columns:\n",
    "        print(\"varibales are \")\n",
    "        print(sorted(different_columns)) \n",
    "    else:\n",
    "        print(\"all variables fine\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b29e3dc",
   "metadata": {},
   "source": [
    "### drop 'YPerchDen', updateMasterTable, and join to lake csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7692a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files saved with row number unchanged\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "master_table = pd.read_excel(\"MasterTable_Distinct.xlsx\")\n",
    "\n",
    "if 'YPerchDen' in master_table.columns:\n",
    "    master_table.drop(columns=['YPerchDen'], inplace=True)\n",
    "\n",
    "master_table_unique = master_table.drop_duplicates(subset=[\"tifffile\", \"csvfile\"])  # 不保留 first, 现在应该唯一\n",
    "\n",
    "simc_data = pd.read_csv(\"combined_filtered_data_SIMC.csv\")\n",
    "huron_data = pd.read_csv(\"combined_filtered_data_HURON.csv\")\n",
    "\n",
    "merged_simc = simc_data.merge(master_table_unique, \n",
    "                              left_on=[\"Image.File\", \"file_name\"], \n",
    "                              right_on=[\"tifffile\", \"csvfile\"], \n",
    "                              how=\"left\")\n",
    "\n",
    "merged_huron = huron_data.merge(master_table_unique, \n",
    "                                left_on=[\"Image.File\", \"file_name\"], \n",
    "                                right_on=[\"tifffile\", \"csvfile\"], \n",
    "                                how=\"left\")\n",
    "\n",
    "merged_simc.drop(columns=[\"tifffile\", \"csvfile\"], inplace=True)\n",
    "merged_huron.drop(columns=[\"tifffile\", \"csvfile\"], inplace=True)\n",
    "\n",
    "# check row number does not change after adding MasterTable variables\n",
    "assert len(merged_simc) == len(simc_data), \"row number unchanged\"\n",
    "assert len(merged_huron) == len(huron_data), \"row number changed\"\n",
    "\n",
    "merged_simc.to_csv(\"SIMC_Merged_With_Master_YPerchDen_Drop.csv\", index=False)\n",
    "merged_huron.to_csv(\"HURON_Merged_With_Master_YPerchDen_Drop.csv\", index=False)\n",
    "\n",
    "print(\"files saved with row number unchanged\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c155805",
   "metadata": {},
   "source": [
    "## remove rows with no MasterTable information (use 'SITE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42ff6aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file: SIMC_Merged_With_Master_YPerchDen_Drop.csv\n",
      "original #row: 413196\n",
      "updated #row: 390530\n",
      "saved as: Cleaned_SIMC_Merged_With_Master_YPerchDen_Drop.csv\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_52/2046231751.py:4: DtypeWarning: Columns (105,107,108,109,125,128) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file: HURON_Merged_With_Master_YPerchDen_Drop.csv\n",
      "original #row: 93205\n",
      "updated #row: 51429\n",
      "saved as: Cleaned_HURON_Merged_With_Master_YPerchDen_Drop.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "files = [\"SIMC_Merged_With_Master_YPerchDen_Drop.csv\", \"HURON_Merged_With_Master_YPerchDen_Drop.csv\"]\n",
    "\n",
    "for file in files:\n",
    "    df = pd.read_csv(file)\n",
    "    original_rows = len(df)\n",
    "    \n",
    "    df_cleaned = df.dropna(subset=[\"SITE\"])\n",
    "    df_cleaned = df_cleaned[df_cleaned[\"SITE\"] != \"\"]\n",
    "\n",
    "    cleaned_rows = len(df_cleaned)\n",
    "\n",
    "    cleaned_file = f\"Cleaned_{file}\"\n",
    "    df_cleaned.to_csv(cleaned_file, index=False)\n",
    "\n",
    "    print(f\"file: {file}\")\n",
    "    print(f\"original #row: {original_rows}\")\n",
    "    print(f\"updated #row: {cleaned_rows}\")\n",
    "    print(f\"saved as: {cleaned_file}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efeb7cb7",
   "metadata": {},
   "source": [
    "## variable list check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e85b8d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file : MasterTable_Distinct.xlsx\n",
      "# row: 17075\n",
      "variables (39): ['tifffile', 'csvfile', 'Year', 'SAM', 'Month', 'Day', 'Rep', 'repnum', 'Key', 'Loc', 'SITE', 'DOY', 'gdd2', 'WaterT', 'LAT0', 'LAT1', 'LON0', 'LON1', 'avgdepth', 'trawltime', 'EFFSPEED', 'MinDepth', 'MaxDepth', 'XANGLE', 'PRECIP', 'XWAVEHT', 'WIND', 'CLOUD_PC', 'distshore', 'FR', 'volbest', 'WhitefishDen', 'UnknwCoregonine', 'CiscoDen', 'Exposure', 'SmeltDen', 'YPerchDen', 'BurbotDen', 'OtherFishDen']\n"
     ]
    }
   ],
   "source": [
    "file_path = \"MasterTable_Distinct.xlsx\"  \n",
    "df = pd.read_excel(file_path) \n",
    "\n",
    "columns = df.columns.tolist()\n",
    "num_entries = df.shape[0]\n",
    "\n",
    "print(f\"file : {file_path}\")\n",
    "print(f\"# row: {num_entries}\")\n",
    "print(f\"variables ({len(columns)}): {columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "093b15ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file : Cleaned_HURON_Merged_With_Master_YPerchDen_Drop.csv\n",
      "# row: 51429\n",
      "variables (137): ['file_name', 'Class.Particle.ID', 'Class', 'Area..ABD.', 'Area..Filled.', 'Aspect.Ratio', 'Calibration.Factor', 'Calibration.Image', 'Camera', 'Capture.X', 'Capture.Y', 'Circle.Fit', 'Circularity', 'Circularity..Hu.', 'Compactness', 'Convex.Perimeter', 'Convexity', 'Date', 'Diameter..ABD.', 'Diameter..ESD.', 'Diameter..FD.', 'Edge.Gradient', 'Elapsed.Time', 'Elongation', 'Feret.Angle.Max', 'Feret.Angle.Min', 'Fiber.Curl', 'Fiber.Straightness', 'Filter.Score', 'Geodesic.Aspect.Ratio', 'Geodesic.Length', 'Geodesic.Thickness', 'Image.File', 'Image.Height', 'Image.Width', 'Image.X', 'Image.Y', 'Intensity', 'Length', 'Original.Reference.ID', 'Particles.Per.Chain', 'Perimeter', 'Roughness', 'Sigma.Intensity', 'Source.Image', 'Sphere.Complement', 'Sphere.Count', 'Sphere.Unknown', 'Sphere.Volume', 'Sum.Intensity', 'Symmetry', 'Time', 'Timestamp', 'Transparency', 'Volume..ABD.', 'Volume..ESD.', 'Width', 'Particle.ID', 'Biovolume..Cylinder.', 'Biovolume..P..Spheroid.', 'Biovolume..Sphere.', 'Particle ID', 'Area (ABD)', 'Area (Filled)', 'Aspect Ratio', 'Calibration Factor', 'Calibration Image', 'Capture X', 'Capture Y', 'Circle Fit', 'Circularity (Hu)', 'Convex Perimeter', 'Diameter (ABD)', 'Diameter (ESD)', 'Diameter (FD)', 'Edge Gradient', 'Elapsed Time', 'Feret Angle Max', 'Feret Angle Min', 'Fiber Curl', 'Fiber Straightness', 'Filter Score', 'Geodesic Aspect Ratio', 'Geodesic Length', 'Geodesic Thickness', 'Image File', 'Image Height', 'Image Width', 'Image X', 'Image Y', 'Original Reference ID', 'Particles Per Chain', 'Sigma Intensity', 'Source Image', 'Sphere Complement', 'Sphere Count', 'Sphere Unknown', 'Sphere Volume', 'Sum Intensity', 'Volume (ABD)', 'Volume (ESD)', 'Year', 'SAM', 'Month', 'Day', 'Rep', 'repnum', 'Key', 'Loc', 'SITE', 'DOY', 'gdd2', 'WaterT', 'LAT0', 'LAT1', 'LON0', 'LON1', 'avgdepth', 'trawltime', 'EFFSPEED', 'MinDepth', 'MaxDepth', 'XANGLE', 'PRECIP', 'XWAVEHT', 'WIND', 'CLOUD_PC', 'distshore', 'FR', 'volbest', 'WhitefishDen', 'UnknwCoregonine', 'CiscoDen', 'Exposure', 'SmeltDen', 'BurbotDen', 'OtherFishDen']\n"
     ]
    }
   ],
   "source": [
    "file_path = \"combined_filtered_data_HURON.csv\"  \n",
    "df = pd.read_csv(file_path) \n",
    "\n",
    "columns = df.columns.tolist()\n",
    "num_entries = df.shape[0]\n",
    "\n",
    "print(f\"file : {file_path}\")\n",
    "print(f\"# row: {num_entries}\")\n",
    "print(f\"variables ({len(columns)}): {columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b15c33ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file : combined_filtered_data_SIMC.csv\n",
      "# row: 413196\n",
      "variables (61): ['file_name', 'Class.Particle.ID', 'Class', 'Area..ABD.', 'Area..Filled.', 'Aspect.Ratio', 'Calibration.Factor', 'Calibration.Image', 'Camera', 'Capture.X', 'Capture.Y', 'Circle.Fit', 'Circularity', 'Circularity..Hu.', 'Compactness', 'Convex.Perimeter', 'Convexity', 'Date', 'Diameter..ABD.', 'Diameter..ESD.', 'Diameter..FD.', 'Edge.Gradient', 'Elapsed.Time', 'Elongation', 'Feret.Angle.Max', 'Feret.Angle.Min', 'Fiber.Curl', 'Fiber.Straightness', 'Filter.Score', 'Geodesic.Aspect.Ratio', 'Geodesic.Length', 'Geodesic.Thickness', 'Image.File', 'Image.Height', 'Image.Width', 'Image.X', 'Image.Y', 'Intensity', 'Length', 'Original.Reference.ID', 'Particles.Per.Chain', 'Perimeter', 'Roughness', 'Sigma.Intensity', 'Source.Image', 'Sphere.Complement', 'Sphere.Count', 'Sphere.Unknown', 'Sphere.Volume', 'Sum.Intensity', 'Symmetry', 'Time', 'Timestamp', 'Transparency', 'Volume..ABD.', 'Volume..ESD.', 'Width', 'Particle.ID', 'Biovolume..Cylinder.', 'Biovolume..P..Spheroid.', 'Biovolume..Sphere.']\n"
     ]
    }
   ],
   "source": [
    "file_path = \"combined_filtered_data_SIMC.csv\"  \n",
    "df = pd.read_csv(file_path) \n",
    "\n",
    "columns = df.columns.tolist()\n",
    "num_entries = df.shape[0]\n",
    "\n",
    "print(f\"file : {file_path}\")\n",
    "print(f\"# row: {num_entries}\")\n",
    "print(f\"variables ({len(columns)}): {columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc73e7f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file : Cleaned_HURON_Merged_With_Master_YPerchDen_Drop.csv\n",
      "# row: 51429\n",
      "variables (137): ['file_name', 'Class.Particle.ID', 'Class', 'Area..ABD.', 'Area..Filled.', 'Aspect.Ratio', 'Calibration.Factor', 'Calibration.Image', 'Camera', 'Capture.X', 'Capture.Y', 'Circle.Fit', 'Circularity', 'Circularity..Hu.', 'Compactness', 'Convex.Perimeter', 'Convexity', 'Date', 'Diameter..ABD.', 'Diameter..ESD.', 'Diameter..FD.', 'Edge.Gradient', 'Elapsed.Time', 'Elongation', 'Feret.Angle.Max', 'Feret.Angle.Min', 'Fiber.Curl', 'Fiber.Straightness', 'Filter.Score', 'Geodesic.Aspect.Ratio', 'Geodesic.Length', 'Geodesic.Thickness', 'Image.File', 'Image.Height', 'Image.Width', 'Image.X', 'Image.Y', 'Intensity', 'Length', 'Original.Reference.ID', 'Particles.Per.Chain', 'Perimeter', 'Roughness', 'Sigma.Intensity', 'Source.Image', 'Sphere.Complement', 'Sphere.Count', 'Sphere.Unknown', 'Sphere.Volume', 'Sum.Intensity', 'Symmetry', 'Time', 'Timestamp', 'Transparency', 'Volume..ABD.', 'Volume..ESD.', 'Width', 'Particle.ID', 'Biovolume..Cylinder.', 'Biovolume..P..Spheroid.', 'Biovolume..Sphere.', 'Particle ID', 'Area (ABD)', 'Area (Filled)', 'Aspect Ratio', 'Calibration Factor', 'Calibration Image', 'Capture X', 'Capture Y', 'Circle Fit', 'Circularity (Hu)', 'Convex Perimeter', 'Diameter (ABD)', 'Diameter (ESD)', 'Diameter (FD)', 'Edge Gradient', 'Elapsed Time', 'Feret Angle Max', 'Feret Angle Min', 'Fiber Curl', 'Fiber Straightness', 'Filter Score', 'Geodesic Aspect Ratio', 'Geodesic Length', 'Geodesic Thickness', 'Image File', 'Image Height', 'Image Width', 'Image X', 'Image Y', 'Original Reference ID', 'Particles Per Chain', 'Sigma Intensity', 'Source Image', 'Sphere Complement', 'Sphere Count', 'Sphere Unknown', 'Sphere Volume', 'Sum Intensity', 'Volume (ABD)', 'Volume (ESD)', 'Year', 'SAM', 'Month', 'Day', 'Rep', 'repnum', 'Key', 'Loc', 'SITE', 'DOY', 'gdd2', 'WaterT', 'LAT0', 'LAT1', 'LON0', 'LON1', 'avgdepth', 'trawltime', 'EFFSPEED', 'MinDepth', 'MaxDepth', 'XANGLE', 'PRECIP', 'XWAVEHT', 'WIND', 'CLOUD_PC', 'distshore', 'FR', 'volbest', 'WhitefishDen', 'UnknwCoregonine', 'CiscoDen', 'Exposure', 'SmeltDen', 'BurbotDen', 'OtherFishDen']\n"
     ]
    }
   ],
   "source": [
    "file_path = \"Cleaned_HURON_Merged_With_Master_YPerchDen_Drop.csv\"  \n",
    "df = pd.read_csv(file_path) \n",
    "\n",
    "columns = df.columns.tolist()\n",
    "num_entries = df.shape[0]\n",
    "\n",
    "print(f\"file : {file_path}\")\n",
    "print(f\"# row: {num_entries}\")\n",
    "print(f\"variables ({len(columns)}): {columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0656bc3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file : Cleaned_SIMC_Merged_With_Master_YPerchDen_Drop.csv\n",
      "# row: 390530\n",
      "variables (97): ['file_name', 'Class.Particle.ID', 'Class', 'Area..ABD.', 'Area..Filled.', 'Aspect.Ratio', 'Calibration.Factor', 'Calibration.Image', 'Camera', 'Capture.X', 'Capture.Y', 'Circle.Fit', 'Circularity', 'Circularity..Hu.', 'Compactness', 'Convex.Perimeter', 'Convexity', 'Date', 'Diameter..ABD.', 'Diameter..ESD.', 'Diameter..FD.', 'Edge.Gradient', 'Elapsed.Time', 'Elongation', 'Feret.Angle.Max', 'Feret.Angle.Min', 'Fiber.Curl', 'Fiber.Straightness', 'Filter.Score', 'Geodesic.Aspect.Ratio', 'Geodesic.Length', 'Geodesic.Thickness', 'Image.File', 'Image.Height', 'Image.Width', 'Image.X', 'Image.Y', 'Intensity', 'Length', 'Original.Reference.ID', 'Particles.Per.Chain', 'Perimeter', 'Roughness', 'Sigma.Intensity', 'Source.Image', 'Sphere.Complement', 'Sphere.Count', 'Sphere.Unknown', 'Sphere.Volume', 'Sum.Intensity', 'Symmetry', 'Time', 'Timestamp', 'Transparency', 'Volume..ABD.', 'Volume..ESD.', 'Width', 'Particle.ID', 'Biovolume..Cylinder.', 'Biovolume..P..Spheroid.', 'Biovolume..Sphere.', 'Year', 'SAM', 'Month', 'Day', 'Rep', 'repnum', 'Key', 'Loc', 'SITE', 'DOY', 'gdd2', 'WaterT', 'LAT0', 'LAT1', 'LON0', 'LON1', 'avgdepth', 'trawltime', 'EFFSPEED', 'MinDepth', 'MaxDepth', 'XANGLE', 'PRECIP', 'XWAVEHT', 'WIND', 'CLOUD_PC', 'distshore', 'FR', 'volbest', 'WhitefishDen', 'UnknwCoregonine', 'CiscoDen', 'Exposure', 'SmeltDen', 'BurbotDen', 'OtherFishDen']\n"
     ]
    }
   ],
   "source": [
    "file_path = \"Cleaned_SIMC_Merged_With_Master_YPerchDen_Drop.csv\"  \n",
    "df = pd.read_csv(file_path) \n",
    "\n",
    "columns = df.columns.tolist()\n",
    "num_entries = df.shape[0]\n",
    "\n",
    "print(f\"file : {file_path}\")\n",
    "print(f\"# row: {num_entries}\")\n",
    "print(f\"variables ({len(columns)}): {columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d711e3d",
   "metadata": {},
   "source": [
    "## variable list (not finalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1dc57603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54\n"
     ]
    }
   ],
   "source": [
    "geometric_features = ['Area..ABD.', 'Area..Filled.', 'Diameter..ABD.', 'Diameter..ESD.', 'Diameter..FD.',\n",
    "                      'Length','Width', 'Perimeter', 'Volume..ABD.', 'Volume..ESD.', 'Geodesic.Length', \n",
    "                      'Geodesic.Thickness']\n",
    "\n",
    "shape_features = ['Aspect.Ratio', 'Circle.Fit', 'Circularity', 'Circularity..Hu.', 'Compactness', \n",
    "                  'Convex.Perimeter', 'Convexity', 'Fiber.Curl', 'Fiber.Straightness', \n",
    "                  'Geodesic.Aspect.Ratio', 'Intensity', 'Roughness', 'Elongation', 'Symmetry']\n",
    "\n",
    "optical_features = ['Edge.Gradient', 'Intensity','Sigma.Intensity', 'Sum.Intensity', 'Transparency']\n",
    "\n",
    "environmental_features = ['gdd2', 'WaterT', 'avgdepth', 'MinDepth', 'MaxDepth', 'CLOUD_PC', 'PRECIP', \n",
    "                          'distshore', 'Exposure', 'XANGLE', 'XWAVEHT']\n",
    "\n",
    "sampling_features = ['SITE', 'Loc', 'LAT0', 'LAT1', 'LON0', 'LON1']\n",
    "\n",
    "biological_features = ['WhitefishDen', 'UnknwCoregonine', 'CiscoDen', 'SmeltDen', 'BurbotDen', \n",
    "                       'OtherFishDen']\n",
    "\n",
    "sum_features = geometric_features + shape_features + optical_features + environmental_features + sampling_features + biological_features\n",
    "\n",
    "print(len(sum_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f37e39",
   "metadata": {},
   "source": [
    "## HURON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3d5749e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as `HURON_Predictor_Selection_Dataset.csv`\n",
      "variable count 57\n",
      "row count: 51429\n"
     ]
    }
   ],
   "source": [
    "file_path = \"Cleaned_HURON_Merged_With_Master_YPerchDen_Drop.csv\"  \n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "columns_to_keep = ['file_name', 'Image.File', 'Class'] + sum_features  \n",
    "df = df[columns_to_keep]\n",
    "\n",
    "output_file = \"HURON_Predictor_Selection_Dataset.csv\"\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"saved as `{output_file}`\")\n",
    "print(f\"variable count {len(df.columns)}\")\n",
    "print(f\"row count: {len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b18aa5",
   "metadata": {},
   "source": [
    "## SIMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90a85d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as `SIMC_Predictor_Selection_Dataset.csv`\n",
      "variable count 57\n",
      "row count: 390530\n"
     ]
    }
   ],
   "source": [
    "file_path = \"Cleaned_SIMC_Merged_With_Master_YPerchDen_Drop.csv\"  \n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "columns_to_keep = ['file_name', 'Image.File', 'Class'] + sum_features  \n",
    "df = df[columns_to_keep]\n",
    "\n",
    "output_file = \"SIMC_Predictor_Selection_Dataset.csv\"\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"saved as `{output_file}`\")\n",
    "print(f\"variable count {len(df.columns)}\")\n",
    "print(f\"row count: {len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806b4d01",
   "metadata": {},
   "source": [
    "# Datasets for Predictor Selection\n",
    "# \"HURON_Predictor_Selection_Dataset.csv\"\n",
    "# \"SIMC_Predictor_Selection_Dataset.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "333f139b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files duplicated successfully as final_HURON.csv and final_SIMC.csv\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.copy(\"HURON_Predictor_Selection_Dataset.csv\", \"final_HURON.csv\")\n",
    "shutil.copy(\"SIMC_Predictor_Selection_Dataset.csv\", \"final_SIMC.csv\")\n",
    "\n",
    "print(\"Files duplicated successfully as final_HURON.csv and final_SIMC.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
