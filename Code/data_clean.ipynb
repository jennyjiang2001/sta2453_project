{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e21b2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bd1402",
   "metadata": {},
   "source": [
    "## target_classes for the classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a2490c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_classes = [\"Calanoid_1\", \"Cyclopoid_1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7e3095",
   "metadata": {},
   "source": [
    "## retain target classes observations and add file name in the first column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7db0fd68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files have been processed and saved to ./combined_filtered_data_HURON.csv!\n",
      "All files have been processed and saved to ./combined_filtered_data_SIMC.csv!\n"
     ]
    }
   ],
   "source": [
    "def combine_filtered_data(input_folder, output_file, target_classes):\n",
    "    combined_data = pd.DataFrame()\n",
    "\n",
    "    for file_name in os.listdir(input_folder):\n",
    "        input_file_path = os.path.join(input_folder, file_name)\n",
    "\n",
    "        if not file_name.endswith(\".csv\"):\n",
    "            continue\n",
    "\n",
    "        data = pd.read_csv(input_file_path)\n",
    "\n",
    "        filtered_data = data[data[\"Class\"].isin(target_classes)]\n",
    "\n",
    "        filtered_data.insert(0, 'file_name', file_name)\n",
    "\n",
    "        combined_data = pd.concat([combined_data, filtered_data], ignore_index=True)\n",
    "\n",
    "    combined_data.to_csv(output_file, index=False)\n",
    "    print(f\"All files have been processed and saved to {output_file}!\")\n",
    "\n",
    "combine_filtered_data(\"./HURONOvlerap_csv\", \"./combined_filtered_data_HURON.csv\", target_classes)\n",
    "combine_filtered_data(\"./SIMCOverlap_csv\", \"./combined_filtered_data_SIMC.csv\", target_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a120aaa1",
   "metadata": {},
   "source": [
    "## keep distinct entries in MasterTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8bb62c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# rows in original MasterTable: 17807 \n",
      "# distinct rows in updated MasterTable: 17075 \n"
     ]
    }
   ],
   "source": [
    "file_path = \"MasterTable_AI_FlowCAM.xlsx\"\n",
    "master_table = pd.read_excel(file_path)\n",
    "\n",
    "master_table_distinct = master_table.drop_duplicates()\n",
    "\n",
    "output_file = \"MasterTable_Distinct.xlsx\"\n",
    "master_table_distinct.to_excel(output_file, index=False)\n",
    "\n",
    "print(f\"# rows in original MasterTable: {len(master_table)} \")\n",
    "print(f\"# distinct rows in updated MasterTable: {len(master_table_distinct)} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918b641d",
   "metadata": {},
   "source": [
    "## use \"tifffile & csvfile\" as key to add MasterTable variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "547f99a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "find 692 repeated 'tifffile & csvfile' combinations\n",
      "repeated 'tifffile & csvfile' combinations saved to 'Duplicate_Tifffile_Csvfile_Records.csv'\n"
     ]
    }
   ],
   "source": [
    "file_path = \"MasterTable_Distinct.xlsx\"\n",
    "master_table = pd.read_excel(file_path)\n",
    "\n",
    "duplicate_groups = master_table.groupby([\"tifffile\", \"csvfile\"]).size().reset_index(name=\"count\")\n",
    "\n",
    "duplicates = duplicate_groups[duplicate_groups[\"count\"] > 1]\n",
    "\n",
    "if duplicates.empty:\n",
    "    print(\"all 'tifffile & csvfile' combinations are unique\")\n",
    "else:\n",
    "    print(f\"find {len(duplicates)} repeated 'tifffile & csvfile' combinations\")\n",
    "\n",
    "    duplicate_rows = master_table[master_table.duplicated(subset=[\"tifffile\", \"csvfile\"], keep=False)]\n",
    "    duplicate_rows_with_index = duplicate_rows.reset_index() \n",
    "\n",
    "    duplicate_rows_with_index.to_csv(\"Duplicate_Tifffile_Csvfile_Records.csv\", index=False)\n",
    "    print(\"repeated 'tifffile & csvfile' combinations saved to 'Duplicate_Tifffile_Csvfile_Records.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf62426",
   "metadata": {},
   "source": [
    "## find variables with different entries in unique combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d16ee3eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "find 692 repeated 'tifffile & csvfile' combinations\n",
      "varibales are \n",
      "['YPerchDen']\n"
     ]
    }
   ],
   "source": [
    "file_path = \"MasterTable_Distinct.xlsx\"\n",
    "master_table = pd.read_excel(file_path)\n",
    "\n",
    "duplicate_groups = master_table.groupby([\"tifffile\", \"csvfile\"]).size().reset_index(name=\"count\")\n",
    "duplicates = duplicate_groups[duplicate_groups[\"count\"] > 1]\n",
    "\n",
    "if duplicates.empty:\n",
    "    print(\"all 'tifffile & csvfile' combinations are unique\")\n",
    "else:\n",
    "    print(f\"find {len(duplicates)} repeated 'tifffile & csvfile' combinations\")\n",
    "\n",
    "    duplicate_rows = master_table[master_table.duplicated(subset=[\"tifffile\", \"csvfile\"], keep=False)]\n",
    "\n",
    "    grouped = duplicate_rows.groupby([\"tifffile\", \"csvfile\"])\n",
    "\n",
    "    different_columns = set()\n",
    "    \n",
    "    for (tifffile, csvfile), group in grouped:\n",
    "        group = group.drop(columns=[\"tifffile\", \"csvfile\"]).reset_index(drop=True)\n",
    "        \n",
    "        for col in group.columns:\n",
    "            if group[col].nunique() > 1:  \n",
    "                different_columns.add(col)\n",
    "\n",
    "    if different_columns:\n",
    "        print(\"varibales are \")\n",
    "        print(sorted(different_columns)) \n",
    "    else:\n",
    "        print(\"all variables fine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b29e3dc",
   "metadata": {},
   "source": [
    "## drop 'YPerchDen', updateMasterTable, and join to lake csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7692a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files saved with row number unchanged\n"
     ]
    }
   ],
   "source": [
    "master_table = pd.read_excel(\"MasterTable_Distinct.xlsx\")\n",
    "\n",
    "if 'YPerchDen' in master_table.columns:\n",
    "    master_table.drop(columns=['YPerchDen'], inplace=True)\n",
    "\n",
    "master_table_unique = master_table.drop_duplicates(subset=[\"tifffile\", \"csvfile\"])   \n",
    "\n",
    "simc_data = pd.read_csv(\"combined_filtered_data_SIMC.csv\")\n",
    "huron_data = pd.read_csv(\"combined_filtered_data_HURON.csv\")\n",
    "\n",
    "merged_simc = simc_data.merge(master_table_unique, \n",
    "                              left_on=[\"Image.File\", \"file_name\"], \n",
    "                              right_on=[\"tifffile\", \"csvfile\"], \n",
    "                              how=\"left\")\n",
    "\n",
    "merged_huron = huron_data.merge(master_table_unique, \n",
    "                                left_on=[\"Image.File\", \"file_name\"], \n",
    "                                right_on=[\"tifffile\", \"csvfile\"], \n",
    "                                how=\"left\")\n",
    "\n",
    "merged_simc.drop(columns=[\"tifffile\", \"csvfile\"], inplace=True)\n",
    "merged_huron.drop(columns=[\"tifffile\", \"csvfile\"], inplace=True)\n",
    "\n",
    "# check row number does not change after adding MasterTable variables\n",
    "assert len(merged_simc) == len(simc_data), \"row number unchanged\"\n",
    "assert len(merged_huron) == len(huron_data), \"row number changed\"\n",
    "\n",
    "merged_simc.to_csv(\"SIMC_Merged_With_Master_YPerchDen_Drop.csv\", index=False)\n",
    "merged_huron.to_csv(\"HURON_Merged_With_Master_YPerchDen_Drop.csv\", index=False)\n",
    "\n",
    "print(\"files saved with row number unchanged\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c155805",
   "metadata": {},
   "source": [
    "## remove rows with no MasterTable information (use 'SITE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42ff6aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file: SIMC_Merged_With_Master_YPerchDen_Drop.csv\n",
      "original #row: 413196\n",
      "updated #row: 390530\n",
      "saved as: Cleaned_SIMC_Merged_With_Master_YPerchDen_Drop.csv\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_52/2046231751.py:4: DtypeWarning: Columns (105,107,108,109,125,128) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file: HURON_Merged_With_Master_YPerchDen_Drop.csv\n",
      "original #row: 93205\n",
      "updated #row: 51429\n",
      "saved as: Cleaned_HURON_Merged_With_Master_YPerchDen_Drop.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "files = [\"SIMC_Merged_With_Master_YPerchDen_Drop.csv\", \"HURON_Merged_With_Master_YPerchDen_Drop.csv\"]\n",
    "\n",
    "for file in files:\n",
    "    df = pd.read_csv(file)\n",
    "    original_rows = len(df)\n",
    "    \n",
    "    df_cleaned = df.dropna(subset=[\"SITE\"])\n",
    "    df_cleaned = df_cleaned[df_cleaned[\"SITE\"] != \"\"]\n",
    "\n",
    "    cleaned_rows = len(df_cleaned)\n",
    "\n",
    "    cleaned_file = f\"Cleaned_{file}\"\n",
    "    df_cleaned.to_csv(cleaned_file, index=False)\n",
    "\n",
    "    print(f\"file: {file}\")\n",
    "    print(f\"original #row: {original_rows}\")\n",
    "    print(f\"updated #row: {cleaned_rows}\")\n",
    "    print(f\"saved as: {cleaned_file}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f37e39",
   "metadata": {},
   "source": [
    "## prepare final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0f775c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54\n"
     ]
    }
   ],
   "source": [
    "geometric_features = ['Area..ABD.', 'Area..Filled.', 'Diameter..ABD.', 'Diameter..ESD.', 'Diameter..FD.',\n",
    "                      'Length','Width', 'Perimeter', 'Volume..ABD.', 'Volume..ESD.', 'Geodesic.Length', \n",
    "                      'Geodesic.Thickness']\n",
    "\n",
    "shape_features = ['Aspect.Ratio', 'Circle.Fit', 'Circularity', 'Circularity..Hu.', 'Compactness', \n",
    "                  'Convex.Perimeter', 'Convexity', 'Fiber.Curl', 'Fiber.Straightness', \n",
    "                  'Geodesic.Aspect.Ratio', 'Intensity', 'Roughness', 'Elongation', 'Symmetry']\n",
    "\n",
    "optical_features = ['Edge.Gradient', 'Intensity','Sigma.Intensity', 'Sum.Intensity', 'Transparency']\n",
    "\n",
    "environmental_features = ['gdd2', 'WaterT', 'avgdepth', 'MinDepth', 'MaxDepth', 'CLOUD_PC', 'PRECIP', \n",
    "                          'distshore', 'Exposure', 'XANGLE', 'XWAVEHT']\n",
    "\n",
    "sampling_features = ['SITE', 'Loc', 'LAT0', 'LAT1', 'LON0', 'LON1']\n",
    "\n",
    "biological_features = ['WhitefishDen', 'UnknwCoregonine', 'CiscoDen', 'SmeltDen', 'BurbotDen', \n",
    "                       'OtherFishDen']\n",
    "\n",
    "sum_features = geometric_features + shape_features + optical_features + environmental_features + sampling_features + biological_features\n",
    "\n",
    "print(len(sum_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3d5749e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as `HURON_Predictor_Selection_Dataset.csv`\n",
      "variable count 57\n",
      "row count: 51429\n",
      "saved as `SIMC_Predictor_Selection_Dataset.csv`\n",
      "variable count 57\n",
      "row count: 390530\n"
     ]
    }
   ],
   "source": [
    "file_path = \"Cleaned_HURON_Merged_With_Master_YPerchDen_Drop.csv\"  \n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "columns_to_keep = ['file_name', 'Image.File', 'Class'] + sum_features  \n",
    "df = df[columns_to_keep]\n",
    "\n",
    "output_file = \"HURON_Predictor_Selection_Dataset.csv\"\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"saved as `{output_file}`\")\n",
    "print(f\"variable count {len(df.columns)}\")\n",
    "print(f\"row count: {len(df)}\")\n",
    "\n",
    "file_path = \"Cleaned_SIMC_Merged_With_Master_YPerchDen_Drop.csv\"  \n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "columns_to_keep = ['file_name', 'Image.File', 'Class'] + sum_features  \n",
    "df = df[columns_to_keep]\n",
    "\n",
    "output_file = \"SIMC_Predictor_Selection_Dataset.csv\"\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"saved as `{output_file}`\")\n",
    "print(f\"variable count {len(df.columns)}\")\n",
    "print(f\"row count: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "333f139b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files duplicated successfully as final_HURON.csv and final_SIMC.csv\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.copy(\"HURON_Predictor_Selection_Dataset.csv\", \"final_HURON.csv\")\n",
    "shutil.copy(\"SIMC_Predictor_Selection_Dataset.csv\", \"final_SIMC.csv\")\n",
    "\n",
    "print(\"Files duplicated successfully as final_HURON.csv and final_SIMC.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09489110",
   "metadata": {},
   "source": [
    "## split for training, validation, and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75acc5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "SEED = 77 \n",
    "\n",
    "file_huron = \"final_HURON.csv\"\n",
    "file_simc = \"final_SIMC.csv\"\n",
    "\n",
    "df_huron = pd.read_csv(file_huron)\n",
    "df_simc = pd.read_csv(file_simc)\n",
    "\n",
    "target_col = 'Class'  \n",
    "\n",
    "def encode_target(df):\n",
    "    df[target_col] = df[target_col].map({\"Calanoid_1\": 0, \"Cyclopoid_1\": 1})\n",
    "    return df\n",
    "\n",
    "df_huron = encode_target(df_huron)\n",
    "df_simc = encode_target(df_simc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8dfe5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_save(df, lake_name):\n",
    "    print(f\"\\nSplitting dataset for {lake_name}...\")\n",
    "\n",
    "    X = df.drop(columns=[target_col])  \n",
    "    y = df[[target_col]]  \n",
    "    \n",
    "    X_train, X_rest, y_train, y_rest = train_test_split(\n",
    "        X, y, test_size=0.4, random_state=SEED, shuffle=True\n",
    "    )\n",
    "\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_rest, y_rest, test_size=0.5, random_state=SEED, shuffle=True\n",
    "    )\n",
    "\n",
    "    X_train.to_csv(f\"{lake_name}_input_train.csv\", index=False)\n",
    "    X_val.to_csv(f\"{lake_name}_input_validate.csv\", index=False)\n",
    "    X_test.to_csv(f\"{lake_name}_input_test.csv\", index=False)\n",
    "\n",
    "    y_train.to_csv(f\"{lake_name}_output_train.csv\", index=False)\n",
    "    y_val.to_csv(f\"{lake_name}_output_validate.csv\", index=False)\n",
    "    y_test.to_csv(f\"{lake_name}_output_test.csv\", index=False)\n",
    "\n",
    "    print(f\"{lake_name} dataset split completed and saved!\")\n",
    "\n",
    "split_and_save(df_huron, \"HURON\")\n",
    "split_and_save(df_simc, \"SIMC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ec1593",
   "metadata": {},
   "source": [
    "## check missing entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f138548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing Value Percentage for final_HURON.csv:\n",
      "CiscoDen     23.270917\n",
      "distshore    23.121196\n",
      "dtype: float64\n",
      "\n",
      "Missing Value Percentage for final_SIMC.csv:\n",
      "distshore       12.847669\n",
      "Exposure         2.842803\n",
      "WhitefishDen     1.650577\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "predictors = {\n",
    "    \"HURON\": [\n",
    "        'Area..Filled.', 'Diameter..FD.', 'Length', 'Width', 'LON0', 'Transparency',\n",
    "        'Volume..ESD.', 'MinDepth', 'LAT0', 'Aspect.Ratio', 'CiscoDen', 'Circularity',\n",
    "        'WaterT', 'Intensity', 'Symmetry', 'Roughness', 'gdd2', 'CLOUD_PC', 'Geodesic.Length',\n",
    "        'Compactness', 'Elongation', 'Perimeter', 'Volume..ABD.', 'Edge.Gradient', \n",
    "        'Convex.Perimeter', 'Convexity', 'Fiber.Straightness', 'Fiber.Curl', 'PRECIP', \n",
    "        'distshore', 'XANGLE'\n",
    "    ],\n",
    "    \"SIMC\": [\n",
    "        'Area..ABD.', 'LON0', 'Length', 'Width', 'MaxDepth', 'Transparency', 'Symmetry', \n",
    "        'WaterT', 'Aspect.Ratio', 'Diameter..ABD.', 'Compactness', 'Elongation', 'Roughness', \n",
    "        'Convex.Perimeter', 'Intensity', 'Fiber.Straightness', 'Circularity', 'Volume..ESD.', \n",
    "        'Volume..ABD.', 'gdd2', 'Perimeter', 'Geodesic.Length', 'Edge.Gradient', 'WhitefishDen', \n",
    "        'LAT0', 'XANGLE', 'PRECIP', 'distshore', 'Exposure', 'Convexity', 'Fiber.Curl'\n",
    "    ]\n",
    "}\n",
    "\n",
    "file_paths = {\n",
    "    \"HURON\": \"final_HURON.csv\",\n",
    "    \"SIMC\": \"final_SIMC.csv\"\n",
    "}\n",
    "\n",
    "for key, file in file_paths.items():\n",
    "    df = pd.read_csv(file) \n",
    "    selected_columns = predictors[key]  \n",
    "    \n",
    "    missing_percentage = df[selected_columns].isnull().mean() * 100  \n",
    "\n",
    "    missing_percentage = missing_percentage[missing_percentage > 0]  \n",
    "\n",
    "    print(f\"\\nMissing Value Percentage for {file}:\")\n",
    "    print(missing_percentage.sort_values(ascending=False))  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
